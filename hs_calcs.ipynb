{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T14:18:59.495772Z",
     "start_time": "2024-03-27T14:18:59.473486Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T14:18:59.513555Z",
     "start_time": "2024-03-27T14:18:59.481787Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function for reading in esm data\n",
    "def get_esms_and_memebers_with_scenarios(directory, filename, scenarios):\n",
    "    data = pd.read_csv(os.path.join(directory, filename))\n",
    "    my_obj = data[(data['year'] == 2050) & \n",
    "        (data['acronym'] == 'NWN') &\n",
    "        (data['experiment'].isin(scenarios))].groupby('ensemble')['experiment'].apply(lambda x: len(np.unique(x)))\n",
    "    members = my_obj.keys()\n",
    "    number_of_my_scenarios = my_obj.values\n",
    "    indeces = np.where(number_of_my_scenarios == 4)[0]\n",
    "    member = None\n",
    "    if len(indeces) > 0:\n",
    "        index = indeces[0]\n",
    "        member = members[index]\n",
    "    \n",
    "    return data['esm'].values[0], member, data['variable'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T14:19:03.603576Z",
     "start_time": "2024-03-27T14:18:59.486373Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to time series esm data\n",
    "data_path = '/Users/snyd535/Documents/GitHub/SnyderEtAl2023_uncertainty_informed_curation_metarepo/extracted_timeseries'\n",
    "#'extracted_timeseries'\n",
    "directory = os.path.join(data_path, 'land_regions_ts')\n",
    "\n",
    "# The scenarios we want to be included\n",
    "scenarios = ['ssp126', 'ssp245', 'ssp370', 'ssp585']\n",
    "\n",
    "esm_arr = [None] * len(os.listdir(directory))\n",
    "member_arr = [None] * len(os.listdir(directory))\n",
    "variable_arr = [None] * len(os.listdir(directory))\n",
    "for i, filename in enumerate(os.listdir(directory)):\n",
    "    try:\n",
    "        esm, member, variable = get_esms_and_memebers_with_scenarios(directory, filename, scenarios)\n",
    "        esm_arr[i] = esm\n",
    "        member_arr[i] = member\n",
    "        variable_arr[i] = variable\n",
    "    except: \n",
    "        pass\n",
    "\n",
    "# Convert data into pandas\n",
    "full_collection = pd.DataFrame({'esm': esm_arr, 'ensemble': member_arr, 'variable': variable_arr})\n",
    "full_collection = full_collection.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions For Fitting Polynomial and Extracting Needed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T14:19:03.603805Z",
     "start_time": "2024-03-27T14:19:03.601915Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get results for a given scenario (slash model)\n",
    "def get_scenario_results(data, hist_data, ref_tas, experiment, variable, esm):\n",
    "    # Different value column name for pr and tas\n",
    "    value_column = 'ann_agg'\n",
    "    if variable == 'pr':\n",
    "        value_column = 'pr'\n",
    "\n",
    "    # Get data specific to this scenario\n",
    "    experiment_data = data[data['experiment'] == experiment].reset_index(drop=True)\n",
    "\n",
    "    # Calculate tas anomaly from reference year\n",
    "    experiment_data['ann_anomaly'] = experiment_data[value_column].copy() - ref_tas\n",
    "\n",
    "    # print(f'{esm} {experiment} has {len(hist_data.year.values)} historical years and {len(experiment_data.year.values)} future years')\n",
    "\n",
    "    # Fit data to 4th degree polynomial\n",
    "    years = np.concatenate([hist_data.year.values, experiment_data.year.values])\n",
    "    temps = np.concatenate([hist_data.ann_anomaly.values, experiment_data.ann_anomaly.values])\n",
    "    coeffs = np.polyfit(years, temps, 4)\n",
    "    # Evaluate with the found coefficients\n",
    "    smooth_fit = np.polyval(coeffs, years)\n",
    "    # Extract residuals\n",
    "    residuals = temps - smooth_fit\n",
    "\n",
    "    # Return dictionary of results\n",
    "    return {'time_series': temps, 'fit': smooth_fit, 'residuals': residuals}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T14:19:03.608213Z",
     "start_time": "2024-03-27T14:19:03.606431Z"
    }
   },
   "outputs": [],
   "source": [
    "# General results for a given ESM\n",
    "def get_model_results(esm, experiment_data, scenarios, variable, uniform_weighting=True):\n",
    "    # Different value column name for pr and tas\n",
    "    value_column = 'ann_agg'\n",
    "    if variable == 'pr':\n",
    "        value_column = 'pr'\n",
    "\n",
    "    # Get model data\n",
    "    data = experiment_data[experiment_data['esm'] == esm].reset_index(drop=True)\n",
    "\n",
    "    # Get historical data\n",
    "    hist_data = data[data['experiment'] == 'historical'].reset_index(drop=True)\n",
    "\n",
    "    # avg tas over historic period\n",
    "    ref_tas = hist_data[value_column].values.mean().copy()\n",
    "    # ref_tas = hist_data[hist_data['year'] == 2014][value_column].values[0]\n",
    "\n",
    "    # Calculating tas anomaly over the historical period\n",
    "    hist_data['ann_anomaly'] = hist_data[value_column].copy() - ref_tas\n",
    "\n",
    "    # Smooth polynomial fit over historic period\n",
    "    coeffs = np.polyfit(hist_data['year'].values, hist_data['ann_anomaly'].values, 4)\n",
    "    # Evaluate with the found coefficients\n",
    "    smooth_fit = np.polyval(coeffs, hist_data['year'].values)\n",
    "\n",
    "    # Model warming over historic period\n",
    "    model_warming = smooth_fit[-1] - smooth_fit[0]\n",
    "\n",
    "    # Get model weight\n",
    "    #\n",
    "    if uniform_weighting:\n",
    "        model_weight = 1 # Equal weighting for all models\n",
    "    else:\n",
    "         model_weight = 1 / (ref_warming + np.abs(model_warming - ref_warming))\n",
    "\n",
    "\n",
    "    # Create model dictionary\n",
    "    model_dict = {'esm': esm, 'reference_tas': ref_tas, 'weight': model_weight}\n",
    "\n",
    "    # Get results for each scenario and add it to the model's dictionary\n",
    "    for exp in scenarios:\n",
    "        scenario_dict = get_scenario_results(data, hist_data, ref_tas, exp, variable, esm)\n",
    "        model_dict[exp] = scenario_dict\n",
    "    \n",
    "    # Return model results as dictionary\n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalized Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T14:19:03.619491Z",
     "start_time": "2024-03-27T14:19:03.609159Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_normalized_weights(full_dict):\n",
    "    weight_sum = 0\n",
    "    for esm, data in full_dict.items():\n",
    "        weight_sum += data['weight']\n",
    "\n",
    "    for esm, data in full_dict.items():\n",
    "        data['weighted_weight'] = data['weight'] / weight_sum\n",
    "    \n",
    "    return full_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Internal Variability\n",
    "$$V=\\sum\\limits_{m}W_{m}\\text{var}_{s,t}(\\varepsilon_{m,s,t})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T14:19:03.619604Z",
     "start_time": "2024-03-27T14:19:03.611585Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_internal_variability(full_dict, scenarios):\n",
    "    V = 0\n",
    "    for esm, data in full_dict.items():\n",
    "        residuals = []\n",
    "        for exp in scenarios:\n",
    "            residuals = np.concatenate([residuals, data[exp]['residuals']])\n",
    "\n",
    "        V += data['weighted_weight'] * np.var(residuals)\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Uncertainty\n",
    "$$M(t)=\\frac{1}{N_{s}}\\sum\\limits_{s}\\text{var}_{m}^{W}(x_{m,s,t})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T14:19:03.619656Z",
     "start_time": "2024-03-27T14:19:03.614238Z"
    }
   },
   "outputs": [],
   "source": [
    "def weighted_variance(values, weights):\n",
    "    \"\"\"\n",
    "    Return the weighted average and standard deviation.\n",
    "\n",
    "    values, weights -- NumPy ndarrays with the same shape.\n",
    "    \"\"\"\n",
    "    average = np.average(values, weights=weights)\n",
    "    variance = np.average((values-average)**2, weights=weights)\n",
    "    return variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T14:19:03.619690Z",
     "start_time": "2024-03-27T14:19:03.616140Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_uncertainty(full_dict, n_years, esms, scenarios):\n",
    "    scenario_inner_vars = np.zeros((len(scenarios), n_years))\n",
    "    j = 0\n",
    "    for exp in scenarios:\n",
    "        model_fits = np.zeros((len(esms), n_years))\n",
    "        model_weights = np.zeros(len(esms))\n",
    "        i = 0\n",
    "        for esm, data in full_dict.items():\n",
    "            model_fits[i, :] = data[exp]['fit']\n",
    "            model_weights[i] = data['weighted_weight']\n",
    "            i += 1\n",
    "        scenario_inner_vars[j,:] = [weighted_variance(model_fits[:,t], model_weights) for t in range(n_years)]\n",
    "        j += 1\n",
    "\n",
    "    M = np.array([np.sum(scenario_inner_vars[:,t])/len(scenarios) for t in range(n_years)])\n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario Uncertainty\n",
    "$$S(t)=\\text{var}_{s}(\\sum\\limits_{m}W_{m}x_{m,s,t})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T14:19:03.623278Z",
     "start_time": "2024-03-27T14:19:03.621498Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_scenario_uncertainty(full_dict, n_years, scenarios):\n",
    "    scenario_inner_sums = np.zeros((len(scenarios), n_years))\n",
    "    iteration = 0\n",
    "    for exp in scenarios:\n",
    "        inner_sum = [0] * n_years\n",
    "        for esm, data in full_dict.items():\n",
    "            inner_sum += data['weighted_weight'] * data[exp]['fit']\n",
    "        scenario_inner_sums[iteration, :] = inner_sum\n",
    "        iteration += 1\n",
    "    S = np.array([np.var(scenario_inner_sums[:,t]) for t in range(n_years)])\n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for running and extracting results for given set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T14:19:03.631908Z",
     "start_time": "2024-03-27T14:19:03.624853Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_and_filter(directory, file, scenarios, ensemble_member, ipcc_region):\n",
    "    print(os.path.join(directory, file))\n",
    "    data = pd.read_csv(os.path.join(directory, file), index_col=False)\n",
    "    esm = data['esm'].values[0]\n",
    "    variable = data['variable'].values[0]\n",
    "    ensemble_member = full_collection[(full_collection['esm'] == esm) & (full_collection['variable'] == variable)]['ensemble'].values[0]\n",
    "\n",
    "    data = data[(data['ensemble'] == ensemble_member) & \n",
    "        (data['experiment'].isin(scenarios + ['historical'])) &\n",
    "        (data['acronym'] == ipcc_region)].copy()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T14:19:03.631996Z",
     "start_time": "2024-03-27T14:19:03.628755Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_results(esms, scenarios, ensemble_member, variable, ipcc_region):\n",
    "    # Input files\n",
    "    input_directory = '/Users/snyd535/Documents/GitHub/SnyderEtAl2023_uncertainty_informed_curation_metarepo/extracted_timeseries/land_regions_ts'\n",
    "    input_files = [f'IPCC_land_regions_{variable}_{esm}_timeseries_1980-2099.csv' for esm in esms]\n",
    "\n",
    "    # Read in data\n",
    "    full_data = pd.concat([read_and_filter(input_directory, file, scenarios, ensemble_member, ipcc_region) for file in input_files]).copy()\n",
    "\n",
    "    # Years in data\n",
    "    years = np.sort(np.unique(full_data['year'].values))\n",
    "    n_years = len(years)\n",
    "\n",
    "    # Get model fits, and weights\n",
    "    full_dict = {}\n",
    "    for esm in esms:\n",
    "        full_dict[esm] = get_model_results(esm, full_data, scenarios, variable)\n",
    "\n",
    "    # Get variation results\n",
    "    full_dict = add_normalized_weights(full_dict).copy()\n",
    "    V = get_internal_variability(full_dict, scenarios)\n",
    "    M = get_model_uncertainty(full_dict, n_years, esms, scenarios)\n",
    "    S = get_scenario_uncertainty(full_dict, n_years, scenarios)\n",
    "\n",
    "    # Plot\n",
    "    total = M + S + V\n",
    "    region_name = full_data['name'].values[0]\n",
    "    df = pd.DataFrame({'year': years, 'Model Uncertainty': M / total, 'Scenario Uncertainty': S / total, 'Inter-Annual Variability': np.repeat(V, n_years) / total})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running HS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T14:19:04.068145Z",
     "start_time": "2024-03-27T14:19:03.632826Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array(['ARP', 'CAF', 'CAR', 'CAU', 'CNA', 'EAS', 'EAU', 'ECA', 'EEU',\n       'ENA', 'ESAF', 'ESB', 'MDG', 'MED', 'NAU', 'NCA', 'NEAF', 'NEN',\n       'NES', 'NEU', 'NSA', 'NWN', 'NWS', 'NZ', 'RAR', 'RFE', 'SAH',\n       'SAM', 'SAS', 'SAU', 'SCA', 'SEA', 'SEAF', 'SES', 'SSA', 'SWS',\n       'TIB', 'WAF', 'WCA', 'WCE', 'WNA', 'WSAF', 'WSB'], dtype=object)"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Decide what the behavior should be if you choose model/scenario/ensemble combo that doesn't exist\n",
    "\n",
    "# Get list of all models in full collection\n",
    "# Note that NorESM2-LM only has all scenarios for tas not pr. See via below command so we also drop\n",
    "esms_full_exp1 = np.unique(full_collection[full_collection['esm'] != 'NorESM2-LM'].esm.values)\n",
    "esms_full_exp2 = np.unique(full_collection[~(full_collection['esm'].isin(['NorESM2-LM',\n",
    "                                                                          'ACCESS-CM2',\n",
    "                                                                          'CESM2-WACCM',\n",
    "                                                                          'CMCC-CM2-SR5',\n",
    "                                                                          'FGOALS-f3-L',\n",
    "                                                                          'INM-CM4-8',\n",
    "                                                                          'MPI-ESM1-2-HR']))].esm.values)\n",
    "\n",
    "\n",
    "# Choose ESMs\n",
    "esms_min_exp1 = ['ACCESS-CM2',   'ACCESS-ESM1-5', 'CMCC-ESM2', 'MRI-ESM2-0', 'GFDL-ESM4']\n",
    "# From Coefficient Min\n",
    "esms_min_exp2 = ['IPSL-CM6A-LR', 'ACCESS-ESM1-5',  'MRI-ESM2-0', 'BCC-CSM2-MR', 'MIROC6']\n",
    "\n",
    "# Choose Scenarios\n",
    "scenarios = ['ssp126', 'ssp245', 'ssp370', 'ssp585']\n",
    "\n",
    "# Choose Ensemble Member\n",
    "ensemble_member = 'r1i1p1f1'\n",
    "\n",
    "# Choose variable (change b/t tas an pr)\n",
    "variable = 'tas'\n",
    "\n",
    "# This gives the names each of the IPCC regions\n",
    "access_esm_data = pd.read_csv(os.path.join(data_path, 'land_regions_ts', 'IPCC_land_regions_tas_ACCESS-ESM1-5_timeseries_1980_2099.csv'))\n",
    "ipcc_land_regions = np.unique(access_esm_data['acronym'].values)\n",
    "ipcc_land_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T14:19:04.191559Z",
     "start_time": "2024-03-27T14:19:04.069864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/snyd535/Documents/GitHub/SnyderEtAl2023_uncertainty_informed_curation_metarepo/extracted_timeseries/land_regions_ts/IPCC_land_regions_tas_ACCESS-CM2_timeseries_1980-2099.csv\n",
      "/Users/snyd535/Documents/GitHub/SnyderEtAl2023_uncertainty_informed_curation_metarepo/extracted_timeseries/land_regions_ts/IPCC_land_regions_tas_ACCESS-ESM1-5_timeseries_1980-2099.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/snyd535/Documents/GitHub/SnyderEtAl2023_uncertainty_informed_curation_metarepo/extracted_timeseries/land_regions_ts/IPCC_land_regions_tas_ACCESS-ESM1-5_timeseries_1980-2099.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[77], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m variable \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtas\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpr\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28miter\u001B[39m, ipcc_region \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(ipcc_land_regions):\n\u001B[0;32m----> 4\u001B[0m         df1 \u001B[38;5;241m=\u001B[39m \u001B[43mget_results\u001B[49m\u001B[43m(\u001B[49m\u001B[43mesms_full_exp1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscenarios\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mensemble_member\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvariable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mipcc_region\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m         df1\u001B[38;5;241m.\u001B[39mto_csv(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(output_data_path, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvariable\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mipcc_region\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_full_collection_exp1.csv\u001B[39m\u001B[38;5;124m'\u001B[39m), index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m      7\u001B[0m         df2 \u001B[38;5;241m=\u001B[39m get_results(esms_full_exp2, scenarios, ensemble_member, variable, ipcc_region)\n",
      "Cell \u001B[0;32mIn[75], line 7\u001B[0m, in \u001B[0;36mget_results\u001B[0;34m(esms, scenarios, ensemble_member, variable, ipcc_region)\u001B[0m\n\u001B[1;32m      4\u001B[0m input_files \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mIPCC_land_regions_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvariable\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mesm\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_timeseries_1980-2099.csv\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m esm \u001B[38;5;129;01min\u001B[39;00m esms]\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Read in data\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m full_data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([read_and_filter(input_directory, file, scenarios, ensemble_member, ipcc_region) \u001B[38;5;28;01mfor\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m input_files])\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Years in data\u001B[39;00m\n\u001B[1;32m     10\u001B[0m years \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msort(np\u001B[38;5;241m.\u001B[39munique(full_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124myear\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues))\n",
      "Cell \u001B[0;32mIn[75], line 7\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      4\u001B[0m input_files \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mIPCC_land_regions_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvariable\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mesm\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_timeseries_1980-2099.csv\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m esm \u001B[38;5;129;01min\u001B[39;00m esms]\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Read in data\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m full_data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([\u001B[43mread_and_filter\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_directory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscenarios\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mensemble_member\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mipcc_region\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m input_files])\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Years in data\u001B[39;00m\n\u001B[1;32m     10\u001B[0m years \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msort(np\u001B[38;5;241m.\u001B[39munique(full_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124myear\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues))\n",
      "Cell \u001B[0;32mIn[74], line 3\u001B[0m, in \u001B[0;36mread_and_filter\u001B[0;34m(directory, file, scenarios, ensemble_member, ipcc_region)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mread_and_filter\u001B[39m(directory, file, scenarios, ensemble_member, ipcc_region):\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;28mprint\u001B[39m(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(directory, file))\n\u001B[0;32m----> 3\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdirectory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfile\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex_col\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m     esm \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mesm\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m      5\u001B[0m     variable \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvariable\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/Documents/GitHub/SnyderEtAl2023_uncertainty_informed_curation_metarepo/venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m   1014\u001B[0m     dialect,\n\u001B[1;32m   1015\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m   1023\u001B[0m )\n\u001B[1;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/SnyderEtAl2023_uncertainty_informed_curation_metarepo/venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:620\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    617\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    619\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[0;32m--> 620\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    622\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[1;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[0;32m~/Documents/GitHub/SnyderEtAl2023_uncertainty_informed_curation_metarepo/venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1620\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m   1617\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1619\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/SnyderEtAl2023_uncertainty_informed_curation_metarepo/venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1880\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m   1879\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1881\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1882\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1883\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1884\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1885\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1886\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1887\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1888\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1889\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1890\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1891\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m~/Documents/GitHub/SnyderEtAl2023_uncertainty_informed_curation_metarepo/venv/lib/python3.9/site-packages/pandas/io/common.py:873\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    868\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    869\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[1;32m    870\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[1;32m    871\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[1;32m    872\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[0;32m--> 873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    874\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    875\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    876\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    877\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    878\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    879\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m    882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/Users/snyd535/Documents/GitHub/SnyderEtAl2023_uncertainty_informed_curation_metarepo/extracted_timeseries/land_regions_ts/IPCC_land_regions_tas_ACCESS-ESM1-5_timeseries_1980-2099.csv'"
     ]
    }
   ],
   "source": [
    "output_data_path = 'hs_output_data'\n",
    "for variable in ['tas', 'pr']:\n",
    "    for iter, ipcc_region in enumerate(ipcc_land_regions):\n",
    "        df1 = get_results(esms_full_exp1, scenarios, ensemble_member, variable, ipcc_region)\n",
    "        df1.to_csv(os.path.join(output_data_path, f'{variable}_{ipcc_region}_full_collection_exp1.csv'), index=False)\n",
    "\n",
    "        df2 = get_results(esms_full_exp2, scenarios, ensemble_member, variable, ipcc_region)\n",
    "        df2.to_csv(os.path.join(output_data_path, f'{variable}_{ipcc_region}_full_collection_exp2.csv'), index=False)\n",
    "\n",
    "        df3 = get_results(esms_min_exp1, scenarios, ensemble_member, variable, ipcc_region)\n",
    "        df3.to_csv(os.path.join(output_data_path, f'{variable}_{ipcc_region}_subset_collection_exp1.csv'), index=False)\n",
    "\n",
    "        df4 = get_results(esms_min_exp2, scenarios, ensemble_member, variable, ipcc_region)\n",
    "        df4.to_csv(os.path.join(output_data_path, f'{variable}_{ipcc_region}_subset_collection_exp2.csv'), index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('stitches_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "230986d229b4a83088d849fb01e13746c2da10ab433b44ddeeba095e5efb26bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
